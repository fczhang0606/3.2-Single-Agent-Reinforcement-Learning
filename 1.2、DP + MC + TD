################################################################################################################################
# https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py  # 左、下、右、上

# https://github.com/boyu-ai/Hands-on-RL
# https://zhuanlan.zhihu.com/p/468235479?utm_id=0
# https://blog.csdn.net/jiebaoshayebuhui/article/details/128487283
# https://zhuanlan.zhihu.com/p/54936262
################################################################################################################################
import gymnasium as gym
from gymnasium.envs.toy_text.frozen_lake import generate_random_map

import copy
import matplotlib.pyplot as plt  # https://matplotlib.org/stable/index.html#
import numpy as np
import random
################################################################################################################################
def equiprobability_policy(env) :

    pi = np.ones( [env.observation_space.n, env.action_space.n] )  # 16x4

    pi = pi * (1 / env.action_space.n)

    return pi
################################################################################################################################
def episode_sample(env, pi) :

    obs, _ = env.reset()

    episode = []
    while True :

        timestep = []
        timestep.append(obs)

        action = np.random.choice(env.action_space.n, p=pi[obs])

        next_obs, reward, terminated, truncated, _ = env.step(action)

        timestep.append(action)
        timestep.append(reward)
        timestep.append(next_obs)

        episode.append(timestep)

        obs = next_obs

        if terminated or truncated :
            break

    return episode
################################################################################################################################
def policy_evaluation_V(env, pi, epsilon=0.10, gamma=0.98, theta=1e-6) :

    old_V = np.zeros(env.observation_space.n)
    new_V = np.zeros(env.observation_space.n)
    old_Q = np.zeros( [env.observation_space.n, env.action_space.n] )
    new_Q = np.zeros( [env.observation_space.n, env.action_space.n] )

    cnt = 0
    while True :
        diff = 0

        for s in range(env.observation_space.n) :
            new_V[s] = 0

            for a in range(env.action_space.n) :
                new_Q[s, a] = 0

                # print(env.P[s][a])
                for p, next_s, r, done in env.P[s][a] :  # if next_s is done
                    new_Q[s, a] += p*(r+gamma*old_V[next_s]*(1-done))

                new_V[s] += pi[s, a]*new_Q[s, a]

            diff += np.abs(new_V[s] - old_V[s])
            old_V[s] = new_V[s]

        # old_V = new_V  # 直接赋值，精度不够？

        cnt += 1

        if diff < theta :
            print(cnt)
            break

    return old_V
################################################################################################################################
def policy_evaluation_Q(env, pi, epsilon=0.10, gamma=0.98, theta=1e-6) :

    old_V = np.zeros(env.observation_space.n)
    new_V = np.zeros(env.observation_space.n)
    old_Q = np.zeros( [env.observation_space.n, env.action_space.n] )
    new_Q = np.zeros( [env.observation_space.n, env.action_space.n] )

    cnt = 0
    while True :
        diff = 0

        for s in range(env.observation_space.n) :
            new_V[s] = 0

            for a in range(env.action_space.n) :
                new_Q[s, a] = 0

                # print(env.P[s][a])
                for p, next_s, r, done in env.P[s][a] :  # if next_s is done
                        new_V[next_s] = 0

                        for next_a in range(env.action_space.n) :
                            new_V[next_s] += pi[next_s, next_a]*old_Q[next_s, next_a]

                        new_Q[s, a] += p*(r+gamma*new_V[next_s]*(1-done))

                diff += np.abs(new_Q[s, a] - old_Q[s, a])
                old_Q[s, a] = new_Q[s, a]

        for s in range(env.observation_space.n) :
            new_V[s] = 0
            for a in range(env.action_space.n) :
                new_V[s] += pi[s, a]*old_Q[s, a]
            old_V[s] = new_V[s]

        cnt += 1

        if diff < theta :
            print(cnt)
            print(old_V)
            break

    return old_Q
################################################################################################################################
def mc_evaluation_V(env, pi, n_episodes=500000, gamma=0.98) :

    new_V = np.zeros(env.observation_space.n)
    cnt_V = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :

        episode = episode_sample(env, pi)
        G = 0

        for j in range( len(episode)-1, -1, -1 ) :  # range(i, j) == i ~ j-1

            (s, a, r, next_s) = episode[j]
            G = r + gamma*G

            cnt_V[s] = cnt_V[s] + 1
            new_V[s] = new_V[s] + (G - new_V[s])/cnt_V[s]

    return new_V
################################################################################################################################
def mc_evaluation_Q(env, pi, n_episodes=500000, gamma=0.98) :

    new_Q = np.zeros( [env.observation_space.n, env.action_space.n] )
    cnt_Q = np.zeros( [env.observation_space.n, env.action_space.n] )

    for i in range(n_episodes) :

        episode = episode_sample(env, pi)
        G = 0

        for j in range( len(episode)-1, -1, -1 ) :  # range(i, j) == i ~ j-1

            (s, a, r, next_s) = episode[j]
            G = r + gamma*G

            cnt_Q[s, a] = cnt_Q[s, a] + 1
            new_Q[s, a] = new_Q[s, a] + (G - new_Q[s, a])/cnt_Q[s, a]

    return new_Q
################################################################################################################################
def TD0_evaluation(env, pi, n_episodes=500000, gamma=0.98, alpha=0.05) :

    new_V = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, pi)

        for j in range( len(episode) ) :
            (s, a, r, next_s) = episode[j]  # expolit all the transitions
            new_V[s] += alpha*(r+gamma*new_V[next_s]-new_V[s])

    return new_V
################################################################################################################################
def TDlambda_evaluation(env, pi, n_episodes=500000, gamma=0.98, TDlambda = 0.98, alpha=0.05) :

    new_V = np.zeros(env.observation_space.n)
    new_T = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, pi)

        for j in range( len(episode) ) :
            (s, a, r, next_s) = episode[j]

            new_T = gamma*TDlambda*new_T
            new_T[s] += 1
            new_V[s] += alpha*(r+gamma*new_V[next_s]-new_V[s])*new_T[s]

    return new_V
################################################################################################################################
def policy_improvement_greedy(env, Q) :

    new_pi = np.zeros( [env.observation_space.n, env.action_space.n] )

    maxQ = np.max(Q, axis=1)
    #print(maxQ)

    for s in range(env.observation_space.n) :
        q_cnt = 0

        for a in range(env.action_space.n) :
            if Q[s, a] == maxQ[s] :
                q_cnt += 1

        for a in range(env.action_space.n) :
            if Q[s, a] == maxQ[s] :
                new_pi[s, a] = 1/q_cnt
            else :
                new_pi[s, a] = 0

    return new_pi
################################################################################################################################
def policy_improvement_epsilon_greedy(env, Q, epsilon=0.10) :

    new_pi = np.zeros( [env.observation_space.n, env.action_space.n] )

    maxQ = np.max(Q, axis=1)
    #print(maxQ)

    for s in range(env.observation_space.n) :
        q_cnt = 0

        for a in range(env.action_space.n) :
            if Q[s, a] == maxQ[s] :
                q_cnt += 1

        for a in range(env.action_space.n) :
            if Q[s, a] == maxQ[s] :
                new_pi[s, a] = epsilon/env.action_space.n + ( (1-epsilon)/q_cnt )
            else :
                new_pi[s, a] = epsilon/env.action_space.n

    return new_pi
################################################################################################################################
def policy_iteration_1(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff =0

        Q      = policy_evaluation_Q(env, old_pi)
        print(Q)

        new_pi = policy_improvement_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def mc_iteration_1(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff =0

        Q      = mc_evaluation_Q(env, old_pi)
        print(Q)

        new_pi = policy_improvement_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def policy_iteration_2(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff = 0

        Q      = policy_evaluation_Q(env, old_pi)
        print(Q)

        # new_pi = policy_improvement_greedy(env, Q)
        new_pi = policy_improvement_epsilon_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def mc_iteration_2(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff = 0

        Q      = mc_evaluation_Q(env, old_pi)
        print(Q)

        # new_pi = policy_improvement_greedy(env, Q)
        new_pi = policy_improvement_epsilon_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def policy_iteration_3(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff = 0

        Q      = policy_evaluation_Q(env, old_pi)
        print(Q)

        # new_pi = policy_improvement_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q)
        print(max(epsilon/np.log(2*cnt), 4e-2))
        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def mc_iteration_3(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff = 0

        Q      = mc_evaluation_Q(env, old_pi)
        print(Q)

        # new_pi = policy_improvement_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q)
        print(max(epsilon/np.log(2*cnt), 4e-2))
        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def policy_iteration_4(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff = 0

        Q      = policy_evaluation_Q(env, old_pi)
        print(Q)

        # new_pi = policy_improvement_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        print(max(epsilon/cnt, 4e-2))
        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def mc_iteration_4(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        cnt += 1

        diff = 0

        Q      = mc_evaluation_Q(env, old_pi)
        print(Q)

        # new_pi = policy_improvement_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q)
        # new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))
        print(max(epsilon/cnt, 4e-2))
        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))
        print(new_pi)

        diff = np.sum(np.abs(new_pi - old_pi))

        old_pi = new_pi

        if diff < theta :
            print(cnt)
            break

    return old_pi
################################################################################################################################
def value_iteration(env, pi, epsilon=0.10, gamma=0.98, theta=1e-6) :
# value_iteration_V/Q = ?

    old_V  = np.zeros(env.observation_space.n)
    new_V  = np.zeros(env.observation_space.n)
    old_Q  = np.zeros( [env.observation_space.n, env.action_space.n] )
    new_Q  = np.zeros( [env.observation_space.n, env.action_space.n] )

    old_pi = copy.deepcopy(pi)
    new_pi = copy.deepcopy(pi)

    cnt = 0
    while True :
        diff = 0


        for s in range(env.observation_space.n) :
            new_V[s] = 0
            for a in range(env.action_space.n) :
                new_Q[s, a] = 0
                # print(env.P[s][a])
                for p, next_s, r, done in env.P[s][a] :  # if next_s is done
                    new_Q[s, a] += p*(r+gamma*old_V[next_s]*(1-done))

            new_V[s] = np.max(new_Q[s])
            diff += np.abs(new_V[s] - old_V[s])
            old_V[s] = new_V[s]
        # old_V = new_V  # 直接赋值，精度不够？


        for s in range(env.observation_space.n) :
            for a in range(env.action_space.n) :
                new_Q[s, a] = 0
                # print(env.P[s][a])
                for p, next_s, r, done in env.P[s][a] :  # if next_s is done
                    new_Q[s, a] += p*(r+gamma*old_V[next_s]*(1-done))
                old_Q[s, a] = new_Q[s, a]


        maxQ = np.max(old_Q, axis=1)
        #print(maxQ)
        for s in range(env.observation_space.n) :
            q_cnt = 0
            for a in range(env.action_space.n) :
                if old_Q[s, a] == maxQ[s] :
                    q_cnt += 1

            for a in range(env.action_space.n) :
                if old_Q[s, a] == maxQ[s] :
                    new_pi[s, a] = 1/q_cnt
                else :
                    new_pi[s, a] = 0
                old_pi[s, a] = new_pi[s, a]


        cnt += 1


        if diff < theta :
            print(cnt)
            break

    print(old_V)
    print(old_Q)
    print(old_pi)

    return old_pi
################################################################################################################################
def policy_test(env, pi, ite_num=3000, ite_len=100) :

    suc_cnt = 0
    for i in range(ite_num) :

        observation, info = env.reset()

        for j in range(ite_len) :

            action = np.random.choice(env.action_space.n, p=pi[observation])

            observation, reward, terminated, truncated, info = env.step(action)

            if terminated or truncated :
                if reward == 1 :
                    suc_cnt += 1
                break

    print(suc_cnt/ite_num)

    return suc_cnt/ite_num
################################################################################################################################
env=gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True)
# gym.make('FrozenLake-v1', desc=generate_random_map(size=8))

# observation, info = env.reset(seed=42)
observation, info = env.reset()
# print(observation)
# print(info)

pi = equiprobability_policy(env)
# print(pi)
################################################################################################################################
print(policy_evaluation_V(env, pi))
print(mc_evaluation_V(env, pi))
print(TD0_evaluation(env, pi))
print(TDlambda_evaluation(env, pi))
################################################################################################################################
V1 = policy_evaluation_V(env, pi)

error1_avg = []
error2_avg = []
error3_avg = []

episodes_ary = np.arange(1000, 100001, 500)
for epis in episodes_ary :

    V2 = mc_evaluation_V(env, pi, epis)

    V3 = TD0_evaluation(env, pi, epis)

    V4 = TDlambda_evaluation(env, pi, epis)

    error1 = np.array(V2 - V1)
    error1_avg.append(np.mean(np.abs(error1)))

    error2 = np.array(V3 - V1)
    error2_avg.append(np.mean(np.abs(error2)))

    error3 = np.array(V4 - V1)
    error3_avg.append(np.mean(np.abs(error3)))

fig = plt.figure()
plt.plot(episodes_ary, error1_avg, label='mc-bs')
plt.plot(episodes_ary, error2_avg, label='td0-bs')
plt.plot(episodes_ary, error3_avg, label='tdn-bs')
plt.xlabel('episodes')
plt.ylabel('average error')
plt.legend()
plt.show()
################################################################################################################################
print(policy_evaluation_Q(env, pi))
print(mc_evaluation_Q(env, pi))
policy_test(env, pi)
################################################################################################################################
policy_test(env, policy_iteration_1(env, pi))
policy_test(env, mc_iteration_1(env, pi))
policy_test(env, policy_iteration_2(env, pi))
policy_test(env, mc_iteration_2(env, pi))
policy_test(env, policy_iteration_3(env, pi))
policy_test(env, mc_iteration_3(env, pi))
policy_test(env, policy_iteration_4(env, pi))
policy_test(env, mc_iteration_4(env, pi))

policy_test(env, value_iteration(env, pi))
################################################################################################################################
env.close()
################################################################################################################################

