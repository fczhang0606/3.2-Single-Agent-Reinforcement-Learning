{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec4ba0-9144-4aeb-baa2-577da2203d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个MDP，都有一套V*/Q*，有多个Pi*对应到值*\n",
    "# 对于单个MDP，从不同Pi出发，可以达到不同的Pi*，但是值*唯一\n",
    "# 不同Pi在MDP下都会有一套稳定的值V/Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98284b-6c48-42ac-b134-3c5e9e87d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py  # 左、下、右、上\n",
    "\n",
    "# https://github.com/boyu-ai/Hands-on-RL\n",
    "\n",
    "# https://zhuanlan.zhihu.com/p/468235479\n",
    "# https://zhuanlan.zhihu.com/p/470994332\n",
    "# https://zhuanlan.zhihu.com/p/473798126\n",
    "\n",
    "# https://blog.csdn.net/jiebaoshayebuhui/article/details/128487283\n",
    "# https://zhuanlan.zhihu.com/p/54936262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60101d89-f329-45df-b4b1-443526f570ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境搭建：\n",
    "# conda create -n RL_Basics\n",
    "# conda activate RL_Basics\n",
    "\n",
    "# conda install python=3.8\n",
    "\n",
    "# https://pytorch.org/get-started/previous-versions/\n",
    "# conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "\n",
    "\n",
    "\n",
    "# conda install ipykernel\n",
    "# conda install platformdirs\n",
    "# pip3 install ipywidgets\n",
    "# pip3 install --upgrade jupyter_core jupyter_client\n",
    "\n",
    "# python -m ipykernel install --user --name RL_Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f0559-ac7a-4e84-bae7-c993dec165a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"gymnasium[all]\"\n",
    "# pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79409741-efa0-42e0-b3d8-878008b7fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt  # https://matplotlib.org/stable/index.html#\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96891a41-387b-4acc-bb7c-ac8396dda00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equiprobability_policy(env) :\n",
    "\n",
    "    pi = np.ones( [env.observation_space.n, env.action_space.n] )  # 16x4\n",
    "\n",
    "    pi = pi * (1/env.action_space.n)\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83a16b-12b4-42a8-8154-6c8b60bd1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_sample(env, pi) :\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    episode = []\n",
    "    while True :\n",
    "        timestep = []\n",
    "\n",
    "        timestep.append(obs)\n",
    "\n",
    "        action = np.random.choice(env.action_space.n, p=pi[obs])\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "        timestep.append(next_obs)\n",
    "        episode.append(timestep)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        if terminated or truncated :\n",
    "            break\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdd80f-a0c8-4660-8a36-1af1c61e6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_V(env, pi, epsilon=0.10, gamma=0.98, theta=1e-6) :\n",
    "\n",
    "    old_V = np.zeros(env.observation_space.n)\n",
    "    new_V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    old_Q = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "    new_Q = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        diff = 0\n",
    "\n",
    "        for s in range(env.observation_space.n) :\n",
    "            new_V[s] = 0\n",
    "\n",
    "            for a in range(env.action_space.n) :\n",
    "                new_Q[s, a] = 0\n",
    "\n",
    "                # print(env.P[s][a])\n",
    "                for p, s_next, r, done in env.P[s][a] :  # if s_next is done\n",
    "                    new_Q[s, a] += p*(r+gamma*old_V[s_next]*(1-done))\n",
    "\n",
    "                new_V[s] += pi[s, a]*new_Q[s, a]\n",
    "\n",
    "            diff += np.abs(new_V[s] - old_V[s])\n",
    "            old_V[s] = new_V[s]\n",
    "        # old_V = new_V  # 直接赋值，精度不够？变量和变量的=，变化会同步\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ab6b5-a425-4905-b93a-f96911c2e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_Q(env, pi, epsilon=0.10, gamma=0.98, theta=1e-6) :\n",
    "\n",
    "    old_V = np.zeros(env.observation_space.n)\n",
    "    new_V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    old_Q = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "    new_Q = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        diff = 0\n",
    "\n",
    "        for s in range(env.observation_space.n) :\n",
    "            new_V[s] = 0\n",
    "\n",
    "            for a in range(env.action_space.n) :\n",
    "                new_Q[s, a] = 0\n",
    "\n",
    "                # print(env.P[s][a])\n",
    "                for p, s_next, r, done in env.P[s][a] :  # if s_next is done\n",
    "                        new_V[s_next] = 0\n",
    "\n",
    "                        for a_next in range(env.action_space.n) :\n",
    "                            new_V[s_next] += pi[s_next, a_next]*old_Q[s_next, a_next]\n",
    "\n",
    "                        new_Q[s, a] += p*(r+gamma*new_V[s_next]*(1-done))\n",
    "\n",
    "                diff += np.abs(new_Q[s, a] - old_Q[s, a])\n",
    "                old_Q[s, a] = new_Q[s, a]\n",
    "\n",
    "        for s in range(env.observation_space.n) :\n",
    "            new_V[s] = 0\n",
    "            for a in range(env.action_space.n) :\n",
    "                new_V[s] += pi[s, a]*old_Q[s, a]\n",
    "            old_V[s] = new_V[s]\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            print(old_V)\n",
    "            break\n",
    "\n",
    "    return old_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102138fa-7e37-4742-8cb8-78745b0fdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_evaluation_V(env, pi, n_episodes=500000, gamma=0.98) :\n",
    "\n",
    "    new_V = np.zeros(env.observation_space.n)\n",
    "    cnt_V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(n_episodes) :\n",
    "\n",
    "        episode = episode_sample(env, pi)\n",
    "        G = 0\n",
    "\n",
    "        for j in range( len(episode)-1, -1, -1 ) :  # range(i, j) == i ~ j-1\n",
    "\n",
    "            (s, a, r, s_next) = episode[j]\n",
    "            G = r + gamma*G\n",
    "\n",
    "            cnt_V[s] = cnt_V[s] + 1\n",
    "            new_V[s] = new_V[s] + (G - new_V[s])/cnt_V[s]\n",
    "\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b863b3-3ab3-49ad-bff0-d993ce462631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_evaluation_Q(env, pi, n_episodes=500000, gamma=0.98) :\n",
    "\n",
    "    new_Q = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "    cnt_Q = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "\n",
    "    for i in range(n_episodes) :\n",
    "\n",
    "        episode = episode_sample(env, pi)\n",
    "        G = 0\n",
    "\n",
    "        for j in range( len(episode)-1, -1, -1 ) :  # range(i, j) == i ~ j-1\n",
    "\n",
    "            (s, a, r, s_next) = episode[j]\n",
    "            G = r + gamma*G\n",
    "\n",
    "            cnt_Q[s, a] = cnt_Q[s, a] + 1\n",
    "            new_Q[s, a] = new_Q[s, a] + (G - new_Q[s, a])/cnt_Q[s, a]\n",
    "\n",
    "    return new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1f9b8-9f96-4f6a-8b7e-39e226bb0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD0_evaluation_V(env, pi, n_episodes=500000, gamma=0.98, alpha=0.05) :\n",
    "\n",
    "    new_V = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(n_episodes) :\n",
    "        episode = episode_sample(env, pi)\n",
    "\n",
    "        for j in range( len(episode) ) :\n",
    "            (s, a, r, s_next) = episode[j]  # expolit all the transitions\n",
    "            new_V[s] += alpha*(r+gamma*new_V[s_next]-new_V[s])\n",
    "\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907d0d3-e747-480f-89a3-661ccca65b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDlambda_evaluation_V(env, pi, n_episodes=500000, gamma=0.98, TDlambda=0.98, alpha=0.05) :\n",
    "\n",
    "    new_V = np.zeros(env.observation_space.n)\n",
    "    new_T = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(n_episodes) :\n",
    "        episode = episode_sample(env, pi)\n",
    "\n",
    "        for j in range( len(episode) ) :\n",
    "            (s, a, r, s_next) = episode[j]\n",
    "\n",
    "            # 和transitions利用的顺序有关？短时间高强度访问，增大学习权重\n",
    "            new_T = gamma*TDlambda*new_T\n",
    "            new_T[s] += 1\n",
    "\n",
    "            new_V[s] += alpha*(r+gamma*new_V[s_next]-new_V[s])*new_T[s]  # 放大了学习程度\n",
    "\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32a9ec-7c32-4601-a9fe-4ce0b071a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_greedy(env, Q) :\n",
    "\n",
    "    new_pi = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "\n",
    "    maxQ = np.max(Q, axis=1)\n",
    "    #print(maxQ)\n",
    "\n",
    "    for s in range(env.observation_space.n) :\n",
    "        q_cnt = 0\n",
    "\n",
    "        for a in range(env.action_space.n) :\n",
    "            if Q[s, a] == maxQ[s] :  # 至少有1\n",
    "                q_cnt += 1\n",
    "\n",
    "        for a in range(env.action_space.n) :\n",
    "            if Q[s, a] == maxQ[s] :\n",
    "                new_pi[s, a] = 1/q_cnt\n",
    "            else :\n",
    "                new_pi[s, a] = 0\n",
    "\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1cd84-438c-458b-a074-5654cc87b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_epsilon_greedy(env, Q, epsilon=0.10) :\n",
    "\n",
    "    new_pi = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "\n",
    "    maxQ = np.max(Q, axis=1)\n",
    "    #print(maxQ)\n",
    "\n",
    "    for s in range(env.observation_space.n) :\n",
    "        q_cnt = 0\n",
    "\n",
    "        for a in range(env.action_space.n) :\n",
    "            if Q[s, a] == maxQ[s] :  # 至少有1\n",
    "                q_cnt += 1\n",
    "\n",
    "        for a in range(env.action_space.n) :\n",
    "            if Q[s, a] == maxQ[s] :\n",
    "                new_pi[s, a] = epsilon/env.action_space.n + ( (1-epsilon)/q_cnt )\n",
    "            else :\n",
    "                new_pi[s, a] = epsilon/env.action_space.n\n",
    "\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc403a9-1f76-40d7-a8ee-04f10f8bdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_1(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = policy_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "        # new_pi >= old_pi\n",
    "        new_pi = policy_improvement_greedy(env, Q)\n",
    "        print(new_pi)\n",
    "\n",
    "        # 在evaluation准确（Pi～Q），和improvement正确（Q～Pi）的条件下\n",
    "        # 会生成Pi*（下一回合，Pi不再发生变化），Pi*会对应到值*\n",
    "\n",
    "        # 是否存在值接近*，但是还没到值*，就improvement到Pi*（必然存在）\n",
    "        # 此时，再对Pi*做evaluation，才可以得到值*\n",
    "\n",
    "        # Pi～Q～Pi模式，检测Pi误差，确保了Pi*时的Q*\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8d94c-80ce-481c-8e92-3897cad9b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_iteration_1(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = mc_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        new_pi = policy_improvement_greedy(env, Q)\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a39ca1-5b9f-49d4-afb4-eccce3b38591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_2(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = policy_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        new_pi = policy_improvement_epsilon_greedy(env, Q)\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653eac0-8011-4803-830b-2d48f13e28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_iteration_2(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = mc_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        new_pi = policy_improvement_epsilon_greedy(env, Q)\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe786f-e447-4835-93b3-adaec61ca54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_3(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = policy_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        print(max(epsilon/np.log(2*cnt), 4e-2))\n",
    "        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbedd9d-5c2e-43e6-8f5b-37085e3e1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_iteration_3(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = mc_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        print(max(epsilon/np.log(2*cnt), 4e-2))\n",
    "        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/np.log(2*cnt), 4e-2))\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92882775-37ef-4c92-a300-9817cb473242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_4(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = policy_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        print(max(epsilon/cnt, 4e-2))\n",
    "        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cecc51-bd49-4ca3-9c3f-c208d7b9603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_iteration_4(env, pi, epsilon=0.10, gamma=0.98, theta=1e-4) :\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        cnt += 1\n",
    "\n",
    "        diff = 0\n",
    "\n",
    "        Q      = mc_evaluation_Q(env, old_pi)\n",
    "        print(Q)\n",
    "\n",
    "        print(max(epsilon/cnt, 4e-2))\n",
    "        new_pi = policy_improvement_epsilon_greedy(env, Q, max(epsilon/cnt, 4e-2))\n",
    "        print(new_pi)\n",
    "\n",
    "        diff = np.sum(np.abs(new_pi - old_pi))\n",
    "\n",
    "        old_pi = new_pi\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4959255-3076-43b5-be44-a3a9fb7d6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, pi, epsilon=0.10, gamma=0.98, theta=1e-6) :\n",
    "\n",
    "    old_V  = np.zeros(env.observation_space.n)\n",
    "    new_V  = np.zeros(env.observation_space.n)\n",
    "\n",
    "    old_Q  = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "    new_Q  = np.zeros( [env.observation_space.n, env.action_space.n] )\n",
    "\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = copy.deepcopy(pi)\n",
    "\n",
    "\n",
    "    cnt = 0\n",
    "    while True :\n",
    "        diff = 0\n",
    "\n",
    "        for s in range(env.observation_space.n) :\n",
    "            new_V[s] = 0\n",
    "            for a in range(env.action_space.n) :\n",
    "                new_Q[s, a] = 0\n",
    "\n",
    "                # print(env.P[s][a])\n",
    "                for p, s_next, r, done in env.P[s][a] :  # if s_next is done\n",
    "                    new_Q[s, a] += p*(r+gamma*old_V[s_next]*(1-done))\n",
    "\n",
    "            new_V[s] = np.max(new_Q[s])\n",
    "\n",
    "            diff += np.abs(new_V[s] - old_V[s])\n",
    "\n",
    "            old_V[s] = new_V[s]\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        if diff < theta :\n",
    "            print(cnt)\n",
    "            break\n",
    "\n",
    "\n",
    "    for s in range(env.observation_space.n) :\n",
    "        for a in range(env.action_space.n) :\n",
    "            new_Q[s, a] = 0\n",
    "\n",
    "            # print(env.P[s][a])\n",
    "            for p, s_next, r, done in env.P[s][a] :  # if s_next is done\n",
    "                new_Q[s, a] += p*(r+gamma*old_V[s_next]*(1-done))\n",
    "            old_Q[s, a] = new_Q[s, a]\n",
    "\n",
    "\n",
    "    maxQ = np.max(old_Q, axis=1)\n",
    "    #print(maxQ)\n",
    "    for s in range(env.observation_space.n) :\n",
    "        q_cnt = 0\n",
    "        for a in range(env.action_space.n) :\n",
    "            if old_Q[s, a] == maxQ[s] :\n",
    "                q_cnt += 1\n",
    "\n",
    "        for a in range(env.action_space.n) :\n",
    "            if old_Q[s, a] == maxQ[s] :\n",
    "                new_pi[s, a] = 1/q_cnt\n",
    "            else :\n",
    "                new_pi[s, a] = 0\n",
    "            old_pi[s, a] = new_pi[s, a]\n",
    "\n",
    "\n",
    "    print(old_V)\n",
    "    print(old_Q)\n",
    "    print(old_pi)\n",
    "\n",
    "    return old_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd3613-4d2e-4284-ad90-c529ef701007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_test(env, pi, ite_num=10000, ite_len=100) :\n",
    "\n",
    "    suc_cnt = 0\n",
    "    for i in range(ite_num) :\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        for j in range(ite_len) :\n",
    "\n",
    "            action = np.random.choice(env.action_space.n, p=pi[obs])\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated :\n",
    "                if reward == 1 :\n",
    "                    suc_cnt += 1\n",
    "                break\n",
    "\n",
    "    print(suc_cnt/ite_num)\n",
    "\n",
    "    return suc_cnt/ite_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a29fd-162a-4b4c-ac96-22f231cb3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "# gym.make('FrozenLake-v1', desc=generate_random_map(size=8))\n",
    "\n",
    "# obs, info = env.reset(seed=42)\n",
    "obs, info = env.reset()\n",
    "# print(obs)\n",
    "# print(info)\n",
    "\n",
    "pi = equiprobability_policy(env)\n",
    "# print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31640253-4a62-4f4b-8c55-2335fa7bbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy_evaluation_V(env, pi))\n",
    "print(mc_evaluation_V(env, pi))\n",
    "print(TD0_evaluation_V(env, pi))\n",
    "print(TDlambda_evaluation_V(env, pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d37265-d6c6-4221-a1c6-f73c12b137b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error21_avg = []\n",
    "error31_avg = []\n",
    "error41_avg = []\n",
    "\n",
    "\n",
    "episodes_ary = np.arange(1000, 500001, 1000)\n",
    "for epis in episodes_ary :\n",
    "\n",
    "    V1 = policy_evaluation_V(env, pi)\n",
    "\n",
    "    V2 = mc_evaluation_V(env, pi, epis)\n",
    "\n",
    "    V3 = TD0_evaluation_V(env, pi, epis)\n",
    "\n",
    "    V4 = TDlambda_evaluation_V(env, pi, epis)\n",
    "\n",
    "    error21 = np.array(V2 - V1)\n",
    "    error21_avg.append(np.mean(np.abs(error21)))\n",
    "\n",
    "    error31 = np.array(V3 - V1)\n",
    "    error31_avg.append(np.mean(np.abs(error31)))\n",
    "\n",
    "    error41 = np.array(V4 - V1)\n",
    "    error41_avg.append(np.mean(np.abs(error41)))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(episodes_ary, error21_avg, label='mc-bs')\n",
    "plt.plot(episodes_ary, error31_avg, label='td0-bs')\n",
    "plt.plot(episodes_ary, error41_avg, label='tdn-bs')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('average error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffead0-c6e1-40dc-ad29-0d4ff6eb0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy_evaluation_Q(env, pi))\n",
    "print(mc_evaluation_Q(env, pi))\n",
    "policy_test(env, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7d652-dcfa-4d2b-ba17-50a71710e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_test(env, policy_iteration_1(env, pi))\n",
    "policy_test(env, mc_iteration_1(env, pi))\n",
    "policy_test(env, policy_iteration_2(env, pi))\n",
    "policy_test(env, mc_iteration_2(env, pi))\n",
    "policy_test(env, policy_iteration_3(env, pi))\n",
    "policy_test(env, mc_iteration_3(env, pi))\n",
    "policy_test(env, policy_iteration_4(env, pi))\n",
    "policy_test(env, mc_iteration_4(env, pi))\n",
    "\n",
    "policy_test(env, value_iteration(env, pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3993cd-588e-41a3-863a-c24244aba597",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Basics",
   "language": "python",
   "name": "rl_basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
