################################################################################################################################
# https://openai.com/

# https://gymnasium.farama.org/
# https://github.com/Farama-Foundation/Gymnasium

# https://gym.openai.com/
# https://github.com/openai/gym
# https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py

# https://blog.csdn.net/jiebaoshayebuhui/article/details/128487283
# https://zhuanlan.zhihu.com/p/54936262
################################################################################################################################
import gymnasium as gym
from gymnasium.envs.toy_text.frozen_lake import generate_random_map

import matplotlib.pyplot as plt  # https://matplotlib.org/stable/index.html#
plt.rcParams['font.sans-serif']=['SimHei']
import numpy as np
import random
################################################################################################################################
def equiprobability_policy(env) :

    pi = np.ones([env.observation_space.n, env.action_space.n])
    # print(pi.shape)  # 16 x 4
    p = 1 / env.action_space.n
    pi = pi * p
    # print(pi)

    return pi
################################################################################################################################
def policy_evaluation(env, policy) :

    ite_cnt = 0
    gamma = 0.98
    threshold = 1e-8

    value_table = np.zeros(env.observation_space.n)  # 终止态=0

    while True :

        ite_cnt += 1
        pre_value_table = value_table[:]

        for state in range(len(value_table)) :
            pi = policy[state]
            # print(pi)

            v=0
            for action in range(env.action_space.n) :
                for p, next_s, r, done in env.P[state][action] :
                    v += pi[action]*p*(r+gamma*pre_value_table[next_s])
            value_table[state]=v

        if np.sum(np.abs(pre_value_table-value_table))<threshold and ite_cnt >= 1000 :
            # print('value achives stable in iteration: ' + str(ite_cnt))
            return value_table
################################################################################################################################
def episode_sample(env, policy) :

    obs, info = env.reset()

    episode = []
    while True :

        timestep = []
        timestep.append(obs)

        action = np.random.choice(env.action_space.n, p=policy[obs])

        next_obs, reward, terminated, truncated, info = env.step(action)

        timestep.append(action)
        timestep.append(reward)
        timestep.append(next_obs)

        episode.append(timestep)

        if terminated or truncated :
            break

        obs = next_obs

    return episode
################################################################################################################################
def mc_evaluation(env, policy, n_episodes=10000) :

    gamma=0.98
    value_table = np.zeros(env.observation_space.n)
    count_table = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, policy)
        G = 0

        for j in range(len(episode)-1, -1, -1) :  # range(i, j) == i ~ j-1
            (s, a, r, s_next) = episode[j]
            G = r + gamma*G

            count_table[s] = count_table[s] + 1
            value_table[s] = value_table[s] + (G-value_table[s])/count_table[s]

    return value_table
################################################################################################################################
def mc_evaluation_Q(env, policy, n_episodes=10000) :

    gamma=0.98
    value_table = np.zeros([env.observation_space.n, env.action_space.n])
    count_table = np.zeros([env.observation_space.n, env.action_space.n])

    for i in range(n_episodes) :
        episode = episode_sample(env, policy)
        G = 0

        for j in range(len(episode)-1, -1, -1) :  # range(i, j) == i ~ j-1
            (s, a, r, s_next) = episode[j]
            G = r + gamma*G

            count_table[s, a] = count_table[s, a] + 1
            value_table[s, a] = value_table[s, a] + (G-value_table[s, a])/count_table[s, a]

    return value_table
################################################################################################################################
def TD0_evaluation(env, policy, n_episodes=10000) :

    alpha=0.05
    gamma=0.98
    value_table = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, policy)

        for j in range(len(episode)) :
            (s, a, r, s_next) = episode[j]
            value_table[s] += alpha*(r+gamma*value_table[s_next]-value_table[s])

    return value_table
################################################################################################################################
def TDlambda_evaluation(env, policy, n_episodes=10000) :

    alpha=0.05
    gamma=0.98
    TDlambda = 0.98
    value_table = np.zeros(env.observation_space.n)
    trace_table = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, policy)

        for j in range(len(episode)) :
            (s, a, r, s_next) = episode[j]
            trace_table = gamma*TDlambda*trace_table
            trace_table[s] += 1
            value_table[s] += alpha*(r+gamma*value_table[s_next]-value_table[s])*trace_table[s]

    return value_table
################################################################################################################################
env=gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True)
# gym.make('FrozenLake-v1', desc=generate_random_map(size=8))

observation, info = env.reset(seed=42)
# print(observation)
# print(info)
################################################################################################################################
pi = equiprobability_policy(env)

value_table1 = policy_evaluation(env, pi)
# print(value_table1)

error1_avg = []
error2_avg = []
error3_avg = []

ite_ary = np.arange(100, 10001, 50)
# print(ite_ary)

for ite in ite_ary :

    value_table2 = mc_evaluation(env, pi, ite)
    # print(value_table2)

    value_table3 = TD0_evaluation(env, pi, ite)
    # print(value_table3)

    value_table4 = TDlambda_evaluation(env, pi, ite)
    # print(value_table4)

    error1 = np.array(value_table2 - value_table1)
    error1_avg.append(np.mean(np.abs(error1)))

    error2 = np.array(value_table3 - value_table1)
    error2_avg.append(np.mean(np.abs(error2)))

    error3 = np.array(value_table4 - value_table1)
    error3_avg.append(np.mean(np.abs(error3)))

################################################################################################################################
fig = plt.figure()
plt.plot(ite_ary, error1_avg, label='mc-bs')
plt.plot(ite_ary, error2_avg, label='td0-bs')
plt.plot(ite_ary, error3_avg, label='tdn-bs')
plt.xlabel('iterations')
plt.ylabel('average error')
plt.legend()
plt.show()
################################################################################################################################
env.close()
################################################################################################################################

