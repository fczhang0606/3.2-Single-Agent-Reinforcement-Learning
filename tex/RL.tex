%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[fleqn]{amsmath}  % https://zhuanlan.zhihu.com/p/464170020
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{geometry}
%\geometry{a4paper, landscape}  % 设置A4纸张并转为横向模式
\usepackage{CJKutf8}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Decision Process}


~ \\[6pt]
\begin{CJK}{UTF8}{gbsn}
    state(S)-policy(pi)-action(A)-model(p)-state(S') \\[6pt]
    reward(R') from transition \\[6pt]
    return(G)  from episode \\[6pt]
    value(V+Q) \\[6pt]
\end{CJK}


\begin{align*}
    p \left( s^{\prime}, r \mid s, a \right)
    = \operatorname{Pr} \left\{ S_{t}=s^{\prime}, R_{t}=r \mid 
    S_{t-1}=s, A_{t-1}=a \right\}
\end{align*}


\begin{align*}
    \sum_{s^{\prime} \in S} \sum_{r \in R} 
    p \left( s^{\prime}, r \mid s, a \right) = 1
\end{align*}


\begin{align*}
    p \left( s^{\prime} \mid s, a \right)
    = \sum_{r \in R} p \left( s^{\prime}, r \mid s, a \right)
\end{align*}


\begin{align*}
    r(s, a) = \sum_{s^{\prime} \in S} 
    \sum_{r \in R} 
    \left( p \left( s^{\prime}, r \mid s, a \right) * r \right)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Bellman Equations}


~ \\[6pt]
\begin{CJK}{UTF8}{gbsn}
    描述状态之间的静态关系 \\[6pt]
\end{CJK}


\begin{align*}
    v_{\pi}(s)
      &= E_{\pi} \left[ G_{t} \mid S_{t}=s \right] \\
      &= E_{\pi} \left[ R_{t+1}+\gamma G_{t+1} \mid S_{t}=s \right] \\
      &= \sum_{a \in A} \pi(a \mid s) 
         \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         E_{\pi} \left[ G_{t+1} \mid S_{t+1}=s^{\prime} \right] \right] \\
      &= \sum_{a \in A} \pi(a \mid s) 
         \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         v_{\pi} \left( s^{\prime} \right) \right] \\
      &= \sum_{a \in A} 
         \left( \pi(a \mid s) * q_{\pi}(s, a) \right)
\end{align*}


\begin{align*}
    q_{\pi}(s, a)
      &= E_{\pi} \left[ G_{t} \mid S_{t}=s, A_{t}=a \right] \\
      &= E_{\pi} \left[ R_{t+1}+\gamma G_{t+1} 
         \mid S_{t}=s, A_{t}=a \right] \\
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         E_{\pi} \left[ G_{t+1} \mid S_{t+1}=s^{\prime} \right] \right] \\
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         v_{\pi} \left( s^{\prime} \right) \right] \\
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         \sum_{a^{\prime} \in A} 
         \left( \pi \left( a^{\prime} \mid s^{\prime} \right) * 
         q_{\pi} \left( s^{\prime}, a^{\prime} \right) \right) \right] \\
\end{align*}


policy-comparison: 
\begin{align*}
    \pi^{\prime} \geq \pi 
    \quad \leftarrow \rightarrow \quad 
    v_{\pi^{\prime}}(s) \geq v_{\pi}(s) 
    \quad \forall s \in S
\end{align*}
\\[6pt]


policy-improvement: 
\begin{align*}
    E_{\pi^{\prime}} 
    \left[ q_{\pi} \left( s, \pi^{\prime}(s) \right) \right] 
    \geq v_{\pi}(s) 
    = E_{\pi} \left[ q_{\pi} \left( s, \pi(s) \right) \right] 
    \quad \forall s \in S
\end{align*}
\\[6pt]


optimal-policy: 
\begin{align*}
    v_{*}(s) = \max_{\pi} v_{\pi}(s)
    = \max_{a \in A} q_{\pi *}(s, a) 
    \quad \forall s \in S
\end{align*}
\\[6pt]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Dynamic Programming}


~ \\[6pt]
\begin{CJK}{UTF8}{gbsn}
    (1) 基于模型 p 的策略 pi 迭代（策略估计、策略改进）： \\[6pt]
\end{CJK}


Policy Evaluation: (matrix solution vs. iteration solution)
\begin{align*}
    v_{k+1}(s) 
    &= {E}_{\pi} \left[ R_{t+1}+\gamma 
    v_{k} \left( S_{t+1} \right) \mid S_{t}=s \right] \\
    &= \sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} 
    p \left( s^{\prime}, r \mid s, a \right) 
    \left[ r+\gamma v_{k} \left( s^{\prime} \right) \right]
\end{align*}
\\[6pt]


Policy Improvement: (greedy)
\begin{align*}
    \pi^{\prime}(s) = \underset{a}{\arg \max} \ q_{\pi}(s, a)
\end{align*}
\\[6pt]


~ \\[6pt]
\begin{CJK}{UTF8}{gbsn}
    (2) 基于模型 p 的价值 v 迭代： \\[6pt]
\end{CJK}


Value Evaluation + Policy Improvement: 
\begin{align*}
    v_{k+1}(s) = \max_{a} 
    \mathbb{E} \left[ R_{t+1}+\gamma 
    v_{k} \left( S_{t+1} \right) \mid S_{t}=s, A_{t}=a \right]
\end{align*}
\\[6pt]


\section{Monte Carlo}


\end{document}

