%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%文档类型
\documentclass{article}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%引入宏包
\usepackage[fleqn]{amsmath}  % https://zhuanlan.zhihu.com/p/464170020
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{geometry}
%\geometry{a4paper, landscape}  % 设置A4纸张并转为横向模式
\usepackage{CJKutf8}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%正文内容
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{ch1: Markov Decision Process}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    Static Concepts: \\[3pt]
    state(S) - policy(pi) - action(A) - model(p) - state(S') \\[3pt]
    reward(R') from transition \\[3pt]
    return(G) from trajectory \\[3pt]
    value(V+Q) \\[3pt]

    ~ \\[3pt]
    策略与价值：\\[3pt]
    ①reward、return、value，用来评估一个策略的好坏。 \\[3pt]
    策略与价值一一对应。 \\[3pt]
    ②价值比较，策略比较，策略改进定理。 \\[3pt]
    ③强化学习的终极目标，求取最优策略， \\[3pt]
    最优策略不唯一，最优价值唯一。 \\[3pt]
    最优动作价值，意味着选取这个动作，未来回报的期望最大。 \\[3pt]
    ④r线性变换，V+Q线性变换，改变最优价值，不改变greedy最优策略。 \\[3pt]
    ⑤迭代时，最优策略可能已经稳定了，但是对应的最优价值还没稳定。 \\[3pt]
    ⑥从终止状态反向迭代更新价值，速度更快。但是哪里是终止状态？上帝视角。 \\[3pt]
\end{CJK}


% 模型p
\begin{align*}
    p \left( s^{\prime}, r \mid s, a \right) 
    = \operatorname{Pr} \left\{ S_{t}=s^{\prime}, R_{t}=r \mid 
    S_{t-1}=s, A_{t-1}=a \right\} 
\end{align*}

% 模型p=1
\begin{align*}
    \sum_{s^{\prime} \in S} \sum_{r \in R} 
    p \left( s^{\prime}, r \mid s, a \right) = 1 
\end{align*}

% 模型p-s'
\begin{align*}
    p \left( s^{\prime} \mid s, a \right) 
    = \sum_{r \in R} p \left( s^{\prime}, r \mid s, a \right) 
\end{align*}

% 奖励r
\begin{align*}
    r(s, a) = \sum_{s^{\prime} \in S} 
    \sum_{r \in R} 
    \left( p \left( s^{\prime}, r \mid s, a \right) * r \right) 
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch2: Bellman Equations}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    Static Relationship \\[3pt]
    实质：描述状态值之间的静态关系（单项形式、矩阵形式） \\[3pt]
    求解：（矩阵求逆、数值迭代）---（policy-evaluation） \\[3pt]
\end{CJK}


% v
\begin{align*}
    v_{\pi}(s) 
      &= E_{\pi} \left[ G_{t} \mid S_{t}=s \right] 
         \qquad \qquad \qquad \qquad \qquad \qquad (Definition) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + \gamma G_{t+1} \mid S_{t}=s \right] 
         \qquad \qquad \qquad \qquad (TD-0) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + R_{t+2} + R_{t+3} + ... 
         \mid S_{t}=s \right] 
         \qquad (TD-n)(TD-\Join =MC) \\[3pt]
      &= \sum_{a \in A} \pi(a \mid s) 
         \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma E_{\pi} 
         \left[ G_{t+1} \mid S_{t+1}=s^{\prime} \right] \right] \\[3pt]
      &= \sum_{a \in A} \pi(a \mid s) 
         \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         v_{\pi} \left( s^{\prime} \right) \right] 
         \qquad (BEs) \\[3pt]
      &= \sum_{a \in A} 
         \pi(a \mid s) * q_{\pi}(s, a) \\[3pt]
\end{align*}

% q
\begin{align*}
    q_{\pi}(s, a) 
      &= E_{\pi} \left[ G_{t} \mid S_{t}=s, A_{t}=a \right] 
      \qquad \qquad \qquad \qquad \qquad \qquad (Definition) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + \gamma G_{t+1} 
         \mid S_{t}=s, A_{t}=a \right] 
         \qquad \qquad \qquad \qquad (TD-0) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + R_{t+2} + R_{t+3} + ... 
         \mid S_{t}=s, A_{t}=a \right] 
         \qquad (TD-n)(TD-\Join =MC) \\[3pt]
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma E_{\pi} 
         \left[ G_{t+1} \mid S_{t+1}=s^{\prime} \right] \right] \\[3pt]
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         v_{\pi} \left( s^{\prime} \right) \right] \\[3pt]
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r \mid s, a \right) * 
         \left[ r + \gamma 
         \sum_{a^{\prime} \in A} 
         \pi ! \left( a^{\prime} \mid s^{\prime} \right) * 
         q_{\pi} \left( s^{\prime}, a^{\prime} \right) \right] 
         \quad (BEs) \\[3pt]
\end{align*}


\newpage


policy-comparison: 
\begin{align*}
    \pi^{\prime} \geq \pi 
    \quad \leftarrow \rightarrow \quad 
    v_{\pi^{\prime}}(s) \geq v_{\pi}(s) 
    \qquad \forall s \in S 
\end{align*}
\\[3pt]


policy-improvement: 
\begin{align*}
    E_{\pi^{\prime}} 
    \left[ q_{\pi} \left( s, \pi^{\prime}(s) \right) \right] 
    \geq v_{\pi}(s) 
    = E_{\pi} \left[ q_{\pi} \left( s, \pi(s) \right) \right] 
    \qquad \forall s \in S 
\end{align*}
\\[3pt]


Bellman Optimal Equations: 
\begin{align*}
    v_{*}(s) 
    & = \max_{\pi} v_{\pi}(s) 
        \qquad \qquad \qquad (Definition) \\[3pt]
    & = \max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v \right) 
        \qquad \qquad (BOEs) \\[3pt]
    & = \max_{a \in A} q_{\pi *}(s, a) 
        \qquad \forall s \in S \\[3pt]
\end{align*}
\\[3pt]


~ \\[3pt]
% 收缩映射定理
\begin{CJK}{UTF8}{gbsn}
    Contraction Mapping Theorem (迭代收敛至唯一不动点) \\[3pt]
    贝尔曼最优方程的收缩迭代过程，即是value iteration算法 \\[3pt]
\end{CJK}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch3: Dynamic Programming}

~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解： \\[3pt]
    Model-based. Dynamics with Model p. \\[3pt]
    ①已知模型p，给定策略Pi，解BEs，得到价值V。 \\[3pt]
    两种解法：矩阵求逆、数值迭代。 \\[3pt]
\end{CJK}


~ \\[3pt]
~ \\[3pt]
(1) Policy Iteration: 
~ \\[3pt]
Policy Evaluation: 
\begin{align*}
    v_{\pi_{k}} = r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}} 
\end{align*}
Policy Improvement: 
\begin{align*}
    \pi_{k+1} = \arg \max_{\pi} 
    ( r_{\pi} + \gamma P_{\pi} v_{k} ) 
\end{align*}


~ \\[3pt]
~ \\[3pt]
(2) Value Iteration: 
\begin{align*}
    v_{k+1} = \max_{\pi} ( r_{\pi} + \gamma P_{\pi} v_{k} ) 
\end{align*}
Policy Update: 
\begin{align*}
    \pi_{k+1} = \arg \max_{\pi} 
    ( r_{\pi} + \gamma P_{\pi} v_{k} ) 
\end{align*}
Value Update: 
\begin{align*}
    v_{k+1} = r_{\pi+1} + \gamma P_{\pi+1} v_{k} 
\end{align*}


~ \\[3pt]
~ \\[3pt]
(3) Turncated Iteration: \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    值迭代有限次数（介于1次与无穷次之间）；值也未稳定，就进行策略改进 \\[3pt]
\end{CJK}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch4: Monte Carlo}


~ \\[3pt]
Sample: 
\begin{align*}
    v_{\pi}(s) 
      &= E_{\pi} \left[ G_{t} \mid S_{t}=s \right] \\[3pt]
    q_{\pi}(s, a) 
      &= E_{\pi} \left[ G_{t} \mid S_{t}=s, A_{t}=a \right] \\[3pt]
\end{align*}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解： \\[3pt]
    Model-free. Dynamics with Trajectory. \\[3pt]
    ①采样进行估计，基于概率论的大数定理。 \\[3pt]
    ②episode长度（探索半径是否覆盖终点？）对估值影响，最优价值是否反向传播。 \\[3pt]
    ③估计的更新方式，非增长式（等着一起算）和增长式（来一个算一个）。 \\[3pt]
    ④epsilon关乎采样策略的探索性和最优性， \\[3pt]
    大则探索性强、最优性弱，小则探索性弱、最优性强， \\[3pt]
    ⑤如果epsilon大到一定程度，可能会导致epsilon-greedy与最优greedy不一致。 \\[3pt]
\end{CJK}


~ \\[3pt]
(1) MC-Basic
~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    二次循环，遍历所有(s, a)；某个策略下，每对采足够样，非增长式估计相应Q。 \\[3pt]
    策略相应的，一套稳定Q值下，策略改进。 \\[3pt]
    迭代。 \\[3pt]
\end{CJK}


~ \\[3pt]
(2) MC-Exploring-Starts
~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    起始分布覆盖(s, a)全集。 \\[3pt]
    Pi下，充分利用每一个trajectory里的所有(s, a)对，访问，即增长式估计相应Q。 \\[3pt]
    每一个trajectory结束后，Q值未必稳定，都进行策略改进。 \\[3pt]
    迭代。 \\[3pt]
\end{CJK}


~ \\[3pt]
(3) MC-epsilon-greedy
~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    过程分布覆盖(s, a)全集。 \\[3pt]
    e-Pi下，充分利用每一个trajectory里的所有(s, a)对，
    访问，即增长式估计相应Q。 \\[3pt]
    每一个trajectory结束后，Q值未必稳定，都进行策略改进，生成e-Pi。 \\[3pt]
    迭代。 \\[3pt]
\end{CJK}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{*stochastic approximation}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    以某形式的公式，为理论依据，进行实际采样与近似估计。 \\[3pt]
\end{CJK}


~ \\[3pt]
(1) Incremental-Estimation: 
\begin{align*}
    w_{k}   &= \frac {1} {k} \sum_{i=1}^{k} x_{i} 
            \qquad \qquad \qquad \qquad 
    w_{k-1}  = \frac {1} {k-1} \sum_{i=1}^{k-1} x_{i} \\[3pt]
    w_{k}   &= \frac {1} {k} \left[ (k-1)w_{k-1} + x_{k} \right] 
            \quad = w_{k-1} + \frac {1} {k} 
            \left[ x_{k} - w_{k-1} \right] 
\end{align*}


~ \\[3pt]
(2) Robbins-Monro: 
\begin{align*}
    g(w) 
    & = 0 \qquad (\text{g is unknown, w is input, 0 is output}) \\[3pt]
    & g(w) = \nabla_{w} L(w) = 0 \quad (\text{w is parametres}) \\[3pt]
    & g(w) = L(w) - C = 0 \\[3pt]
    & \qquad \text{w* is the solution (Convergence Condition)}
\end{align*}
\begin{align*}
    w_{k+1} &= w_{k} + a_{k} \left[ 
            \tilde{g} \left( w_{k}, \eta_{k} \right) - 0 \right] \\[3pt]
            &= w_{k} + a_{k} 
            \left( g(w_{k}) + \eta_{k} \right) \\[3pt]
            & iteration: \ \{w_{k}\} + \{\tilde{g}_{k}\} + \{a_{k}\} 
\end{align*}


~ \\[3pt]
(3) Optimazation: 
\begin{align*}
    & \min_{w} J(w) = E \left[ f(w, X) \right] \\[3pt]
    & \qquad \Rightarrow \Rightarrow \qquad 
    \nabla_{w} E \left[ f(w, X) \right] = 0 \\[3pt]
    & \qquad \Rightarrow \Rightarrow \qquad 
    Gradient: \quad InputSpace, \quad direction + magnitude 
\end{align*}


~ \\[3pt]
GD \quad + \quad (Mini)Batch GD \quad + \quad Stochastic GD: 
\begin{align*}
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \nabla_{w} E \left[ f(w_{k}, X) \right] 
            \quad = \quad w_{k} - \alpha_{k} 
            E \left[ \nabla_{w} f(w_{k}, X) \right] \\[3pt]
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \frac {1} {n} \sum_{i=1}^{n} 
            \nabla_{w} f(w_{k}, x_{i}) \\[3pt]
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \nabla_{w} f(w_{k}, x_{k}) \\[3pt]
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch5: Temporal Difference}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解: \\[3pt]
    Model-free. Dynamics with Transition. \\[3pt]
    ①TD时序差分：在不同时刻，对同一个量估计，有差，利用差改进估计。 \\[3pt]
    ②没有模型p、只有数据t，进行估计： \\[3pt]
    MC利用整条trajectory，估计V、Q，离线/无偏/大方差； \\[3pt]
    TD利用片段transition，估计V、Q，在线/有偏/小方差。 \\[3pt]
    ③SARSA，用TD估计某个Pi的Q。 \\[3pt]
    ④Q-Learning，用TD直接估计Q*。（异轨，提取共性MDP-Q*信息） \\[3pt]
    ⑤行为策略采集数据，使用采集数据，估计目标策略（TD-target的形式）的值。 \\[3pt]
    on-policy vs. off-policy。 \\[3pt]
\end{CJK}


~ \\[3pt]
(1) TD-V: 
\begin{align*}
    v_{t+1} (s_{t}) &= v_{t} (s_{t}) + \alpha_{t} 
        [[r_{t+1} + \gamma v_{t} (s_{t+1})] - v_{t} (s_{t})] \\[3pt]
    v_{t+1} (s)     &= v_{t} (s) 
    \qquad \forall s \neq s_{t} \\[3pt]
\end{align*}


~ \\[3pt]
(2) TD-Q: \qquad SARSA 

~ \\[3pt]
Policy Evaluation: 
\begin{align*}
    q_{t+1} (s_{t}, a_{t}) 
        &= q_{t} (s_{t}, a_{t}) + \alpha_{t} (s_{t}, a_{t}) 
        [ r_{t+1} + \gamma q_{t} (s_{t+1}, a_{t+1}) 
        - q_{t} (s_{t}, a_{t}) ] \\[3pt]
    q_{t+1} (s, a) &= q_{t} (s, a) 
    \qquad \forall (s, a) \neq (s_{t}, a_{t}) 
\end{align*}

~ \\[3pt]
Policy Improvement: \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    Q could be not in the convergence state 
    with the current Pi \\[3pt]
    when the new epsilon-greedy policy generated. \\[3pt]
\end{CJK}


~ \\[3pt]
(3) TD-Q*: \qquad Q-Learning 
\begin{align*}
    q_{t+1} (s_{t}, a_{t}) 
        &= q_{t} (s_{t}, a_{t}) + \alpha_{t} (s_{t}, a_{t}) 
        [ r_{t+1} + \gamma \max_{a} q_{t} (s_{t+1}, a) 
        - q_{t} (s_{t}, a_{t}) ] \\[3pt]
    q_{t+1} (s, a) &= q_{t} (s, a) 
    \qquad \forall (s, a) \neq (s_{t}, a_{t}) 
\end{align*}


\end{document}

