%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%文档类型
\documentclass{article}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%引入宏包
\usepackage[fleqn]{amsmath}  % https://zhuanlan.zhihu.com/p/464170020
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{geometry}
%\geometry{a4paper, landscape}  % 设置A4纸张并转为横向模式
\usepackage{CJKutf8}

\usepackage{booktabs}  % 三线表


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%正文内容
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{ch1: Markov Decision Process}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    Static Concepts: \\[3pt]
    state(S) - policy(pi) - action(A) - model(p) - state(S') \\[3pt]
    reward(R') from transition \\[3pt]
    return(G) from trajectory \\[3pt]
    value(V+Q) \\[3pt]

    ~ \\[3pt]
    策略与价值：\\[3pt]
    ①reward、return、value，用来评估一个策略的好坏。 \\[3pt]
    策略与价值一一对应。 \\[3pt]
    ②价值比较，策略比较，策略改进定理。 \\[3pt]
    ③强化学习的终极目标，求取最优策略， \\[3pt]
    最优策略不唯一，最优价值唯一。 \\[3pt]
    最优动作价值，意味着选取这个动作，未来回报的期望最大。 \\[3pt]
    ④r线性变换，V+Q线性变换，改变最优价值，不改变greedy最优策略。 \\[3pt]
    ⑤迭代时，最优策略可能已经稳定了，但是对应的最优价值还没稳定。 \\[3pt]
    ⑥从终止状态反向迭代更新价值，速度更快。但是哪里是终止状态？上帝视角。 \\[3pt]
\end{CJK}


% 模型p
\begin{align*}
    p \left( s^{\prime}, r | s, a \right) 
    = \operatorname{Pr} \left\{ S_{t}=s^{\prime}, R_{t}=r | 
    S_{t-1}=s, A_{t-1}=a \right\} 
\end{align*}

% 模型p=1
\begin{align*}
    \sum_{s^{\prime} \in S} \sum_{r \in R} 
    p \left( s^{\prime}, r | s, a \right) = 1 
\end{align*}

% 模型p-s'
\begin{align*}
    p \left( s^{\prime} | s, a \right) 
    = \sum_{r \in R} p \left( s^{\prime}, r | s, a \right) 
\end{align*}

% 奖励r
\begin{align*}
    r(s, a) = \sum_{s^{\prime} \in S} 
    \sum_{r \in R} 
    \left( p \left( s^{\prime}, r | s, a \right) * r \right) 
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch2: Bellman Equations}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    Static Relationship \\[3pt]
    实质：描述状态值之间的静态关系（单项形式、矩阵形式） \\[3pt]
    求解：（矩阵求逆、数值迭代）---（policy-evaluation） \\[3pt]
\end{CJK}


% v
\begin{align*}
    v_{\pi}(s) 
      &= E_{\pi} \left[ G_{t} | S_{t}=s \right] 
         \qquad \qquad \qquad \qquad \qquad \qquad (Definition) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + \gamma G_{t+1} | S_{t}=s \right] 
         \qquad \qquad \qquad \qquad (TD-0) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... 
         | S_{t}=s \right] 
         \quad (TD-n)(TD-\Join =MC) \\[3pt]
      &= \sum_{a \in A} \pi(a | s) 
         \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r | s, a \right) * 
         \left[ r + \gamma E_{\pi} 
         \left[ G_{t+1} | S_{t+1}=s^{\prime} \right] \right] \\[3pt]
      &= \sum_{a \in A} \pi(a | s) 
         \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r | s, a \right) * 
         \left[ r + \gamma 
         v_{\pi} \left( s^{\prime} \right) \right] 
         \qquad (BEs) \\[3pt]
      &= \sum_{a \in A} 
         \pi(a | s) * q_{\pi}(s, a) \\[3pt]
\end{align*}

% q
\begin{align*}
    q_{\pi}(s, a) 
      &= E_{\pi} \left[ G_{t} | S_{t}=s, A_{t}=a \right] 
      \qquad \qquad \qquad \qquad \qquad \qquad (Definition) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + \gamma G_{t+1} 
         | S_{t}=s, A_{t}=a \right] 
         \qquad \qquad \qquad \qquad (TD-0) \\[3pt]
      &= E_{\pi} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... 
         | S_{t}=s, A_{t}=a \right] 
         \quad (TD-n)(TD-\Join =MC) \\[3pt]
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r | s, a \right) * 
         \left[ r + \gamma E_{\pi} 
         \left[ G_{t+1} | S_{t+1}=s^{\prime} \right] \right] \\[3pt]
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r | s, a \right) * 
         \left[ r + \gamma 
         v_{\pi} \left( s^{\prime} \right) \right] \\[3pt]
      &= \sum_{s^{\prime}, r} 
         p \left( s^{\prime}, r | s, a \right) * 
         \left[ r + \gamma 
         \sum_{a^{\prime} \in A} 
         \pi ! \left( a^{\prime} | s^{\prime} \right) * 
         q_{\pi} \left( s^{\prime}, a^{\prime} \right) \right] 
         \quad (BEs) \\[3pt]
\end{align*}


\newpage


policy-comparison: 
\begin{align*}
    \pi^{\prime} \geq \pi 
    \quad \leftarrow \rightarrow \quad 
    v_{\pi^{\prime}}(s) \geq v_{\pi}(s) 
    \qquad \forall s \in S 
\end{align*}
\\[3pt]


policy-improvement: 
\begin{align*}
    E_{\pi^{\prime}} 
    \left[ q_{\pi} \left( s, \pi^{\prime}(s) \right) \right] 
    \geq v_{\pi}(s) 
    = E_{\pi} \left[ q_{\pi} \left( s, \pi(s) \right) \right] 
    \qquad \forall s \in S 
\end{align*}
\\[3pt]


Bellman Optimal Equations: 
\begin{align*}
    v_{*}(s) 
    & = \max_{\pi} v_{\pi}(s) 
        \qquad \qquad \qquad (Definition) \\[3pt]
    & = \max_{\pi} \left( r_{\pi} + \gamma P_{\pi} v \right) 
        \qquad \qquad (BOEs) \\[3pt]
    & = \max_{a \in A} q_{\pi *}(s, a) 
        \qquad \forall s \in S \\[3pt]
\end{align*}
\\[3pt]


~ \\[3pt]
% 收缩映射定理
\begin{CJK}{UTF8}{gbsn}
    Contraction Mapping Theorem (迭代收敛至唯一不动点) \\[3pt]
    贝尔曼最优方程的收缩迭代过程，即是value iteration算法 \\[3pt]
\end{CJK}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch3: Dynamic Programming}

~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解： \\[3pt]
    Model-based. Dynamics with Model p. \\[3pt]
    ①已知模型p，给定策略Pi，解BEs，得到价值V。 \\[3pt]
    两种解法：矩阵求逆、数值迭代。 \\[3pt]
\end{CJK}


~ \\[3pt]
~ \\[3pt]
(1) Policy Iteration: 
~ \\[3pt]
Policy Evaluation: 
\begin{align*}
    v_{\pi_{k}} = r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}} 
\end{align*}
Policy Improvement: 
\begin{align*}
    \pi_{k+1} = \arg \max_{\pi} 
    ( r_{\pi} + \gamma P_{\pi} v_{k} ) 
\end{align*}


~ \\[3pt]
~ \\[3pt]
(2) Value Iteration: 
\begin{align*}
    v_{k+1} = \max_{\pi} ( r_{\pi} + \gamma P_{\pi} v_{k} ) 
\end{align*}
Policy Update: 
\begin{align*}
    \pi_{k+1} = \arg \max_{\pi} 
    ( r_{\pi} + \gamma P_{\pi} v_{k} ) 
\end{align*}
Value Update: 
\begin{align*}
    v_{k+1} = r_{\pi+1} + \gamma P_{\pi+1} v_{k} 
\end{align*}


~ \\[3pt]
~ \\[3pt]
(3) Turncated Iteration: \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    值迭代有限次数（介于1次与无穷次之间）；值也未稳定，就进行策略改进 \\[3pt]
\end{CJK}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch4: Monte Carlo}


~ \\[3pt]
Sample: 
\begin{align*}
    v_{\pi}(s) 
      &= E_{\pi} \left[ 
         G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... 
         | S_{t}=s \right] 
         \qquad \qquad \quad (TD-\Join =MC) \\[3pt]
    q_{\pi}(s, a) 
      &= E_{\pi} \left[ 
         G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... 
         | S_{t}=s, A_{t}=a \right] 
         \qquad (TD-\Join =MC) \\[3pt]
\end{align*}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解： \\[3pt]
    Model-free. Dynamics with Trajectory. \\[3pt]
    ①采样进行估计，依据概率论的大数定理。 \\[3pt]
    ②episode长度（探索半径是否覆盖终点？）对估值影响，最优价值是否反向传播。 \\[3pt]
    ③估计的更新方式，非增长式（等着一起算）和增长式（来一个算一个）。 \\[3pt]
    ④epsilon关乎采样策略的探索性和最优性， \\[3pt]
    大则探索性强、最优性弱，小则探索性弱、最优性强， \\[3pt]
    ⑤如果epsilon大到一定程度，可能会导致epsilon-greedy与最优greedy不一致。 \\[3pt]
\end{CJK}


~ \\[3pt]
(1) MC-Basic 
~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    二次循环，遍历所有(s, a)；某个策略下，每对采足够样，非增长式估计相应Q。 \\[3pt]
    策略相应的，一套稳定Q值下，策略改进。 \\[3pt]
    迭代。 \\[3pt]
\end{CJK}


~ \\[3pt]
(2) MC-Exploring-Starts 
~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    起始分布，覆盖(s, a)全集。 \\[3pt]
    Pi下，充分利用每一个trajectory里的所有(s, a)对，访问，增长式估计相应Q。 \\[3pt]
    每一个trajectory结束后，Q值未必稳定，都进行策略改进。 \\[3pt]
    迭代。 \\[3pt]
\end{CJK}


~ \\[3pt]
(3) MC-epsilon-greedy 
~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    过程分布，覆盖(s, a)全集。 \\[3pt]
    e-Pi下，充分利用每一个trajectory里的所有(s, a)对，
    访问，增长式估计相应Q。 \\[3pt]
    每一个trajectory结束后，Q值未必稳定，都进行策略改进，生成e-Pi。 \\[3pt]
    迭代。 \\[3pt]
\end{CJK}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{*stochastic approximation}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解： \\[3pt]
    以某形式的公式，为理论依据，进行实际采样与近似估计。 \\[3pt]
\end{CJK}


~ \\[3pt]
(1) Incremental-Estimation: 
\begin{align*}
    w_{k}   &= \frac {1} {k} \sum_{i=1}^{k} x_{i} 
            \qquad \qquad \qquad \qquad 
    w_{k-1}  = \frac {1} {k-1} \sum_{i=1}^{k-1} x_{i} \\[3pt]
    w_{k}   &= \frac {1} {k} \left[ (k-1)w_{k-1} + x_{k} \right] 
            \quad = w_{k-1} + \frac {1} {k} 
            \left[ x_{k} - w_{k-1} \right] 
\end{align*}


~ \\[3pt]
(2) Robbins-Monro: 
\begin{align*}
    g(w) 
    & = 0 \qquad (\text{g is unknown, w is input, 0 is output}) \\[3pt]
    & g(w) = \nabla_{w} L(w) = 0 \quad (\text{w is parametres}) \\[3pt]
    & g(w) = L(w) - C = 0 \\[3pt]
    & \qquad \text{w* is the solution (Convergence Condition)}
\end{align*}
\begin{align*}
    w_{k+1} &= w_{k} + a_{k} \left[ 
            \tilde{g} \left( w_{k}, \eta_{k} \right) - 0 \right] \\[3pt]
            &= w_{k} + a_{k} 
            \left( g(w_{k}) + \eta_{k} \right) \\[3pt]
            & \qquad iteration: \quad 
            \{w_{k}\} + \{\tilde{g}_{k}\} + \{a_{k}\} 
\end{align*}


~ \\[3pt]
(3) Optimazation: 
\begin{align*}
    & \min_{w} J(w) = E \left[ f(w, X) \right] \\[3pt]
    & \qquad \Rightarrow \Rightarrow \qquad 
    \nabla_{w} E \left[ f(w, X) \right] = 0 \\[3pt]
    & \qquad \Rightarrow \Rightarrow \qquad 
    Gradient: \quad InputSpace, \quad direction + magnitude 
\end{align*}


~ \\[3pt]
GD \quad + \quad (Mini)Batch GD \quad + \quad Stochastic GD: 
\begin{align*}
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \nabla_{w} E \left[ f(w_{k}, X) \right] 
            \quad = \quad w_{k} - \alpha_{k} 
            E \left[ \nabla_{w} f(w_{k}, X) \right] \\[3pt]
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \frac {1} {n} \sum_{i=1}^{n} 
            \nabla_{w} f(w_{k}, x_{i}) \\[3pt]
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \nabla_{w} f(w_{k}, x_{k}) \\[3pt]
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch5: Temporal Difference}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解: \\[5pt]
    Model-free. Dynamics with Transition. \\[5pt]
    ①TD时序差分：在不同时刻，对同一个量估计，有差，利用差改进估计。 \\[5pt]
    ②没有模型p、只有数据t，进行估计： \\[5pt]
    MC利用整条trajectory，估计V、Q，离线/无偏/大方差； \\[5pt]
    TD利用片段transition，估计V、Q，在线/有偏/小方差。 \\[5pt]
    ③SARSA，用TD估计某个Pi的Q。 \\[5pt]
    ④Q-Learning，用TD直接估计Q*。（异轨，从数据中提取共性MDP-Q*信息） \\[5pt]
    ⑤行为策略采集数据，利用数据计算TD-target（数据组织形式）， \\[5pt]
    目标策略的估计值趋向TD-target，即TD-target代表着目标策略。 \\[5pt]
    ⑥on-policy: 用一个策略和环境交互，得到experience， 
    估计这个策略，改进这个策略。 \\[5pt]
    再用改进的策略循环，交互、估计、改进，直至最优（这个系列的最优）。 \\[5pt]
    off-policy: 一个策略和环境交互，得到experience， \\[5pt]
    另一个策略被估计、被改进。（目标策略不易采集数据，所以需要行为策略，异轨） \\[5pt]
    experience应当具有一些性质，能够架通两个策略， \\[5pt]
    行为策略能够采到，目标策略能够使用。 \\[5pt]
\end{CJK}


~ \\[3pt]
(1) TD-V: \qquad s-a-r-s 
\begin{align*}
    v_{t+1} (s_{t}) &= v_{t} (s_{t}) + \alpha_{t} 
        [ [r_{t+1} + \gamma v_{t} (s_{t+1})] - v_{t} (s_{t}) ] \\[3pt]
    v_{t+1} (s)     &= v_{t} (s) 
    \qquad \forall s \neq s_{t} \\[3pt]
\end{align*}

\newpage

~ \\[3pt]
(2) TD-Q: \qquad SA-R-SA 

~ \\[3pt]
Policy Evaluation: 
\begin{align*}
    q_{t+1} (s_{t}, a_{t}) 
        &= q_{t} (s_{t}, a_{t}) + \alpha_{t} (s_{t}, a_{t}) 
        [ r_{t+1} + \gamma q_{t} (s_{t+1}, a_{t+1}) 
        - q_{t} (s_{t}, a_{t}) ] \\[3pt]
    q_{t+1} (s, a) &= q_{t} (s, a) 
    \qquad \forall (s, a) \neq (s_{t}, a_{t}) 
\end{align*}

~ \\[3pt]
Policy Improvement: \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    某个策略下的Q值可能还未估计稳定， \\[3pt]
    就可以进行策略提升。最终目标提升至Q*。 \\[3pt]
\end{CJK}


~ \\[3pt]
(3) TD-Q*: \qquad Q*-Learning \quad (s-a-r-s) 

~ \\[3pt]
Value Improvement: 
\begin{align*}
    q_{t+1} (s_{t}, a_{t}) 
        &= q_{t} (s_{t}, a_{t}) + \alpha_{t} (s_{t}, a_{t}) 
        [ r_{t+1} + \gamma \max_{a} q_{t} (s_{t+1}, a) 
        - q_{t} (s_{t}, a_{t}) ] \\[3pt]
    q_{t+1} (s, a) &= q_{t} (s, a) 
    \qquad \forall (s, a) \neq (s_{t}, a_{t}) 
\end{align*}


~ \\[96pt]
*(DP+MC+TD)-Summary: \\[3pt]
~ \\[3pt]
\centering
\begin{tabular}{ccc}  % centering*3 

    \toprule 
        value & DP-model(p)                 & TD-data(t)  \\[3pt]
    \midrule 
        v     & Policy-Evaluation(vBEs)     & TD(0/n/...) \\[3pt]
        v*    & Value-iteration(vBOEs)      & -           \\[3pt]
        q     & Policy-Evaluation(qBEs)     & TD(0)=SARSA \\[3pt]
        q*    & Value-iteration(qBOEs)      & Q*-Learning \\[3pt]
    \bottomrule 

\end{tabular}
\flushleft


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch6: Value Function Approximation}


~ \\[3pt]
\begin{CJK}{UTF8}{gbsn}
    理解: \\[3pt]
    ①函数拟合的优势：用更少的参数量（存储），估计，更多的状态量（泛化）； \\[3pt]
    原因在于，函数这种表达形式，存储了更本质的信息。 \\[3pt]
    函数拟合的劣势：存在精度损失。 \\[3pt]
    函数拟合的关键：最优的结构 + 最优的参数，提升拟合准确程度。 \\[3pt]
    ②状态分布(.)：平均分布U、稳态分布D（D + (pi+p) = D）。 \\[3pt]
    优化时，SGD将不同分布吸收拉平了。 \\[3pt]
    ③某个真实分布中的一系列真值，形成一个超面； \\[3pt]
    经过各种采样，得到超面的部分观测真值； \\[3pt]
    建立一个模型并优化参数，用来拟合部分观测真值得到的超面。 \\[3pt]
    ④自举，让不均匀的高低估扩展，可能导致最优性发生变化。 \\[3pt]
\end{CJK}


~ \\[3pt]
(1) Objective Function: 
\begin{align*}
    J(w) = E_{S \sim (.)} 
           [ (v_{\pi}(S) - \tilde{v}(S, w) )^2 ] 
\end{align*}

~ \\[3pt]
(2) Optimazation: 
\begin{align*}
    w_{k+1} & 
            \quad = \quad w_{k} - \alpha_{k} 
            \nabla_{w} J(w_{k}) \\[3pt]
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{t} 
            (v_{\pi}(s_{t}) - \tilde{v}(s_{t}; w_{t})) 
            \nabla_{w} \tilde{v}(s_{t}; w_{t}) 
\end{align*}

~ \\[3pt]
(3) Algorithm: 
\begin{align*}
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{t} 
            (g_{t} - \tilde{v}(s_{t}; w_{t})) 
            \nabla_{w} \tilde{v}(s_{t}; w_{t}) 
            \qquad \qquad \qquad \qquad \quad MC+VA \\[3pt]
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{t} 
            [r_{t+1} + \gamma \tilde{v}(s_{t+1}; w_{t}) 
            - \tilde{v}(s_{t}; w_{t})] 
            \nabla_{w} \tilde{v}(s_{t}; w_{t}) 
            \qquad TD+VA 
\end{align*}

~ \\[3pt]
(4) Approximator: 
\begin{align*}
    linear    &= kx + b \\[3pt]
    nonlinear &= neural \ \ network \\[3pt]
\end{align*}


\newpage


~ \\[3pt]
(5) SARSA + VA: \qquad (SA-R-SA + q- + w) 

~ \\[3pt]
Value Update(w): 
\begin{align*}
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{t} 
            [ r_{t+1} + \gamma \tilde{q}(s_{t+1}, a_{t+1}; w_{t}) 
            - \tilde{q}(s_{t}, a_{t}; w_{t}) ] 
            \nabla_{w} \tilde{q}(s_{t}, a_{t}; w_{t}) 
\end{align*}

~ \\[3pt]
Policy Improvement: epsilon-greedy


~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
(6) Q*-Learning + VA: \qquad (S-A-R-S + q- + w) 

~ \\[3pt]
Value Update(w): 
\begin{align*}
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{t} 
            [ r_{t+1} + \gamma \max_{a} \tilde{q}(s_{t+1}, a; w_{t}) 
            - \tilde{q}(s_{t}, a_{t}; w_{t}) ] 
            \nabla_{w} \tilde{q}(s_{t}, a_{t}; w_{t}) 
\end{align*}

~ \\[3pt]
Policy Improvement: (epsilon-)greedy 


~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
(7) Deep Q*-Learning: \qquad (Buffer(S-A-R-S) + NN(w)*2) \\[3pt]
\quad (continuous states + discrete actions)

~ \\[3pt]
Value Update(w): 
\begin{align*}
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{t} 
            [ r_{t+1} + \gamma \max_{a} \tilde{q}(s_{t+1}, a; w_{T}) 
            - \tilde{q}(s_{t}, a_{t}; w_{t}) ] 
            \nabla_{w} \tilde{q}(s_{t}, a_{t}; w_{t}) 
\end{align*}

~ \\[3pt]
Policy Improvement: (epsilon-)greedy 


~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
~ \\[3pt]
(8) Double Q*-Net: \qquad (Buffer(S-A-R-S) + NN(w)*2) 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch7: Policy Gradient}


~ \\[3pt]
*Policy-Summary: \\[3pt]
~ \\[3pt]
\centering
\begin{tabular}{ccc}  % centering*3 

    \toprule 
        & \qquad \qquad \qquad \qquad \qquad \qquad \qquad Policy & \\[3pt]
    \midrule 
        Form          & Distribution        & Network Model         \\[3pt]
        Evaluation    & Values (V + Q)      & Performence Metrics   \\[3pt]
        Optimazation  & (epsilon)-greedy    & Gradient-Descent      \\[3pt]
    \bottomrule 

\end{tabular}
\flushleft


~ \\[3pt]
(1)Policy Network: (Storage + Generalization) 
\begin{align*}
    \pi (a | s; \theta ) 
    \qquad \qquad \qquad \qquad \text{from Distribution to Network Model} 
\end{align*}

~ \\[3pt]
(2)Optimal Policy: (!= Optimal Values) 
\begin{align*}
    \max_{\theta} \ metrics[ \pi (a | s; \theta ) ] 
    \qquad \qquad \text{from Values to Metircs} 
\end{align*}

~ \\[3pt]
(3)Distribution(d): 
\begin{align*}
    d - \pi &: e.g. 
            \qquad Uniform \ Distribution : \qquad \qquad 
            d(s) = 1 / | S | \qquad \forall s \in S \\[3pt]
    d + \pi &: e.g. 
            \qquad Stationary \ Distribution : \qquad \quad \ 
            d_{\pi}^{T} \cdot P_{\pi} = d_{\pi}^{T} \\[3pt]
    d_{0}   &: e.g. 
            \qquad Specific \ Distribution : \qquad \qquad \ 
            d_{0}(s_{0}) = 1 
            \qquad  d_{0}(s) = 0 \qquad \forall s \neq s_{0} 
\end{align*}

~ \\[3pt]
(4)Metric-1: average state value (?) = average average return 
\begin{align*}
    J_{1} (\theta) & \ = \ \overline{v_{\pi}} 
                     \ = \ E_{s \sim d} [ v_{\pi}(S) ] 
                     \ = \ \sum_{s \in d} d(s) v_{\pi}(s) 
                     \ = \ d^{T} \cdot v_{\pi} \\[3pt]
                   & \ = \ E \left[ \sum_{t = 0}^{\infty} 
                                    \gamma^{t} R_{t+1} \right] 
\end{align*}

~ \\[3pt]
(5)Metric-2: average one-step reward = average infinite reward 
\begin{align*}
    J_{2} (\theta) & \ = \ \overline{r_{\pi}} 
                     \ = \ E_{s \sim d} [ r_{\pi}(S) ] 
                     \ = \ \sum_{s \in d} d(s) r_{\pi}(s) \\[3pt]
                   & \ = \ \lim_{n \to \infty} \frac{1}{n} 
                     E \left[ \sum_{k = 1}^{n} R_{t+k} \right] 
\end{align*}

~ \\[3pt]
(6)Metrics-Relationship: 
\begin{align*}
    \overline{r_{\pi}} = (1 - \gamma) \overline{v_{\pi}} 
\end{align*}

~ \\[3pt]
(7)Policy Gradient Theorem: 
\begin{align*}
    \nabla_{\theta} J(\theta) &= \sum_{s \in S} \eta (s) 
    \sum_{a \in A} \nabla_{\theta} \pi (a | s; \theta) q_{\pi} (s, a) 
    \qquad \qquad \qquad \text{proof. Zhang Weinan} \\[3pt]
    &= E \left[ \nabla_{\theta} ln \pi (A | S; \theta) q_{\pi} (S, A) \right] 
    \qquad \qquad \qquad \qquad 
    \text{q: (1)MC   (2)MC-BS   (3)Advantage   (4)TD} \\[3pt]
    \theta_{t+1} &= \theta_{t} + \alpha \nabla_{\theta} 
                    ln \pi (a_{t} | s_{t}; \theta_{t}) q_{\pi} (s_{t}, a_{t}) 
                    \qquad \qquad \qquad \text{stochastic gradient ascent} \\[3pt]
                 &= \theta_{t} + \alpha 
                 \frac{q_{\pi} (s_{t}, a_{t})}{\pi (a_{t} | s_{t}; \theta_{t})} 
                 \nabla_{\theta} \pi (a_{t} | s_{t}; \theta_{t}) 
                 \qquad \qquad \qquad \text{exploitation + exploration} 
\end{align*}

~ \\[3pt]
(8)REINFORCE: 
\begin{align*}
    \theta_{t+1} &= \theta_{t} + \alpha \nabla_{\theta} 
                    ln \pi (a_{t} | s_{t}; \theta_{t}) q_{t} (s_{t}, a_{t}) \\[3pt]
    & q_{\pi} (s_{t}, a_{t}) : \quad q_{t} (s_{t}, a_{t}) (MC) 
\end{align*}


~ \\[3pt]
(9)REINFORCE-BS: 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{ch8: Actor Critic}


~ \\[3pt]
(1) QAC: \ \ \ \ \ {(SARSA-VA) \ + \ PG} 
\begin{align*}
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{w} 
            [ r_{t+1} + \gamma \tilde{q}(s_{t+1}, a_{t+1}; w_{t}) 
            - \tilde{q}(s_{t}, a_{t}; w_{t}) ] 
            \nabla_{w} \tilde{q}(s_{t}, a_{t}; w_{t}) \\[3pt]
    \theta_{t+1} & 
            \quad = \quad \theta_{t} + \alpha_{\theta} \nabla_{\theta} 
            ln \pi (a_{t} | s_{t}; \theta_{t}) \tilde{q}(s_{t}, a_{t}; w_{t+1}) 
\end{align*}


~ \\[3pt]
(2) A2C: \ \ \ \ \ {(TD-VA) \ + \ PG} 
\begin{align*}
    q_{\pi}(s_{t}, a_{t}) - BS & 
            \quad = \quad 
            \tilde{q}(s_{t}, a_{t}; w_{t}^{'}) 
            - \tilde{v}(s_{t}; w_{t}) \\[3pt]
    = \delta_{t} & 
            \quad = \quad 
            r_{t+1} + \gamma \tilde{v}(s_{t+1}; w_{t}) 
            - \tilde{v}(s_{t}; w_{t}) \\[3pt]
    w_{t+1} & 
            \quad = \quad w_{t} + \alpha_{w} \delta_{t} 
            \nabla_{w} \tilde{v}(s_{t}; w_{t}) \\[3pt]
    \theta_{t+1} & 
            \quad = \quad \theta_{t} + \alpha_{\theta} \nabla_{\theta} 
            ln \pi (a_{t} | s_{t}; \theta_{t}) \delta_{t} 
\end{align*}


~ \\[3pt]
Importance Sampling: 
\begin{align*}
    E_{x-p} [f(x)] &= \int_{x} p(x) f(x) \,dx \\[3pt]
                   &= \int_{x} q(x) \frac{p(x)}{q(x)} f(x) \,dx \\[3pt]
                   &= E_{x-q} \left[ \frac{p(x)}{q(x)} f(x) \right] 
                   \qquad \qquad \qquad \qquad 
                   \beta(x) = \frac{p(x)}{q(x)} = \frac{\pi(x)}{\mu(x)} 
\end{align*}


\end{document}

