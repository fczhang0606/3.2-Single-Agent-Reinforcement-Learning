策略与价值一一对应。
强化学习的终极目标，求取最优策略，
最优策略不唯一，最优价值唯一。
最优动作价值，意味着选取这个动作，未来回报的期望最大。
迭代时，最优策略可能已经稳定了，但是对应的最优价值还没稳定。

reward、return、value，评估一个策略的好坏。
策略比较，策略改进定理。

MDP，从终止状态反向更新价值，速度更快。

MC中，episode最大长度（探索半径）
（是否覆盖终点？）对估值精确度的影响。
非增长式和增长式更新。
MC中，epsilon关乎策略的探索性和最优性
如果epsilon较大，会导致epsilon-greedy与最优greedy不一致

with respect to//关于

