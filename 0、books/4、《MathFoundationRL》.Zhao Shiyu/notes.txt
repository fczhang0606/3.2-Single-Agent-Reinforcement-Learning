reward、return、value，评估一个策略的好坏。
策略与价值一一对应。
策略比较，策略改进定理。
强化学习的终极目标，求取最优策略，
最优策略不唯一，最优价值唯一。
最优动作价值，意味着选取这个动作，未来回报的期望最大。
迭代时，最优策略可能已经稳定了，但是对应的最优价值还没稳定。
从终止状态反向更新价值，速度更快。但是哪里是终止状态？开了上帝视角。


更新：非增长式（等着一起算）和增长式（来一个算一个）。
epsilon关乎策略的探索性和最优性，
大则探索性强、最优性弱，小则探索性弱、最优性强，
如果epsilon大到一定程度，可能会导致epsilon-greedy与最优greedy不一致。
episode最大长度（探索半径）
（是否覆盖终点？）对估值精确度的影响。


GD、BGD、SGD、MBGD的联系。
SGD的收敛证明（RM）、性质（收敛方向、速度）。
凸优化，《Convex Optimization》课程。


TD时序差分：在两个时刻，对同一个量的估计的差。利用差改进估计。
在没有模型、只有数据的情况下，进行估计：
MC利用整条trajectory，估计V、Q；
TD利用片段transition，估计V、Q。
SARSA即是用TD估计某个Pi的Q。
Q-Learning利用TD直接估计Bellman*。（异轨，提取共性MDP信息，即Q*信息）


函数拟合的优势：用更少的参数量（存储），估计更多的状态量（泛化）。
函数拟合的劣势：存在精度损失。
函数拟合的关键：最优的结构+最优的参数，提升拟合准确程度。
状态分布：平均分布、稳态分布D(pi+p)。优化时，SGD将分布吸收拉平了。
某个真实分布中的一系列真值，形成一个超面；
经过各种采样，得到超面的部分观测真值；
建立一个模型并优化参数，用来拟合部分观测真值得到的超面。


with respect to//关于

