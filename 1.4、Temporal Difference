################################################################################################################################
# https://openai.com/

# https://gymnasium.farama.org/
# https://github.com/Farama-Foundation/Gymnasium

# https://gym.openai.com/
# https://github.com/openai/gym
# https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py

# https://blog.csdn.net/jiebaoshayebuhui/article/details/128487283
# https://zhuanlan.zhihu.com/p/54936262
################################################################################################################################
import gymnasium as gym
from gymnasium.envs.toy_text.frozen_lake import generate_random_map

import matplotlib.pyplot as plt  # https://matplotlib.org/stable/index.html#
plt.rcParams['font.sans-serif']=['SimHei']
import numpy as np
import random
################################################################################################################################
def equiprobability_policy(env) :

    pi = np.ones([env.observation_space.n, env.action_space.n])
    # print(pi.shape)  # 16 x 4
    p = 1 / env.action_space.n
    pi = pi * p
    # print(pi)

    return pi
################################################################################################################################
def episode_sample(env, policy) :

    obs, info = env.reset()

    episode = []
    while True :

        timestep = []
        timestep.append(obs)

        action = np.random.choice(env.action_space.n, p=policy[obs])

        next_obs, reward, terminated, truncated, info = env.step(action)

        timestep.append(action)
        timestep.append(reward)
        timestep.append(next_obs)

        episode.append(timestep)

        if terminated or truncated :
            break

        obs = next_obs

    return episode
################################################################################################################################
def TD0_evaluation(env, policy, n_episodes=10000) :

    alpha=0.05
    gamma=0.98
    value_table = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, policy)

        for j in range(len(episode)) :
            (s, a, r, s_next) = episode[j]
            value_table[s] += alpha*(r+gamma*value_table[s_next]-value_table[s])

    return value_table
################################################################################################################################
def TDlambda_evaluation(env, policy, n_episodes=10000) :

    alpha=0.05
    gamma=0.98
    TDlambda = 0.98
    value_table = np.zeros(env.observation_space.n)
    trace_table = np.zeros(env.observation_space.n)

    for i in range(n_episodes) :
        episode = episode_sample(env, policy)

        for j in range(len(episode)) :
            (s, a, r, s_next) = episode[j]
            trace_table = gamma*TDlambda*trace_table
            trace_table[s] += 1
            value_table[s] += alpha*(r+gamma*value_table[s_next]-value_table[s])*trace_table[s]

    return value_table
################################################################################################################################
env=gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True)
# gym.make('FrozenLake-v1', desc=generate_random_map(size=8))

observation, info = env.reset(seed=42)
# print(observation)
# print(info)
################################################################################################################################
